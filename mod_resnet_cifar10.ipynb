{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mod_resnet_cifar10",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO0fCXqSiX6fq6oQv/xLRLd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbeniteze/ResNets/blob/master/mod_resnet_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3swsxrGRFDl",
        "colab_type": "code",
        "outputId": "6dd21c01-219b-48e4-b7e0-a9ec7bf6f0f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "!pip3 install keras==2.3.1\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "#device_name = tf.test.gpu_device_name()\n",
        "#if device_name != '/device:GPU:0':\n",
        "#  raise SystemError('GPU device not found')\n",
        "#print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\r\u001b[K     |▉                               | 10kB 27.1MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20kB 34.7MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30kB 42.7MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40kB 30.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51kB 34.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61kB 38.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 71kB 32.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 81kB 34.1MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92kB 36.7MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 112kB 34.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 122kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 133kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 143kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 163kB 34.3MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 174kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 184kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 194kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 204kB 34.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 215kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 225kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 245kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 256kB 34.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 266kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 276kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 286kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 296kB 34.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 307kB 34.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 317kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 327kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 337kB 34.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 348kB 34.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 358kB 34.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 368kB 34.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 378kB 34.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.17.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (2.8.0)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.2.5\n",
            "    Uninstalling Keras-2.2.5:\n",
            "      Successfully uninstalled Keras-2.2.5\n",
            "Successfully installed keras-2.3.1\n",
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZQDhtT1OB1Q",
        "colab_type": "code",
        "outputId": "8cb18421-1257-43e7-9c47-dc07845cd9bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout, Reshape\n",
        "from keras.layers import Flatten, BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "# reshape to be [samples][width][height][channels]\n",
        "X_train = X_train.reshape((X_train.shape[0], 32, 32, 3)).astype('float32')\n",
        "X_test = X_test.reshape((X_test.shape[0], 32, 32, 3)).astype('float32')\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "# one hot encode outputs\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\r     8192/170498071 [..............................] - ETA: 6:58"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVLROem5OLoi",
        "colab_type": "code",
        "outputId": "5cd73b2f-6f90-45f9-b792-5d67c1a1c1dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Flatten, BatchNormalization\n",
        "from keras.layers import Conv2D, Input, Activation, AveragePooling2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "\n",
        "\n",
        "\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='elu',            #modificado\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Dropout(0.05)(x)          #añadido\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Dropout(0.05)(x)          #añadido\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "def resnet_v2(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
        "    bottleneck layer\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'elu'           #modificado\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('elu')(x)           #modificado\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    x = Dropout(0.05)(x)          #añadido    \n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "    \n",
        "model = resnet_v2(input_shape=(32,32,3), depth=56)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_117 (Conv2D)             (None, 32, 32, 16)   448         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_111 (BatchN (None, 32, 32, 16)   64          conv2d_117[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_111 (Dropout)           (None, 32, 32, 16)   0           batch_normalization_111[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_111 (Activation)     (None, 32, 32, 16)   0           dropout_111[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_118 (Conv2D)             (None, 32, 32, 16)   272         activation_111[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_112 (BatchN (None, 32, 32, 16)   64          conv2d_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_112 (Dropout)           (None, 32, 32, 16)   0           batch_normalization_112[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_112 (Activation)     (None, 32, 32, 16)   0           dropout_112[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_119 (Conv2D)             (None, 32, 32, 16)   2320        activation_112[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_113 (BatchN (None, 32, 32, 16)   64          conv2d_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_113 (Dropout)           (None, 32, 32, 16)   0           batch_normalization_113[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_113 (Activation)     (None, 32, 32, 16)   0           dropout_113[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_121 (Conv2D)             (None, 32, 32, 64)   1088        activation_111[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_120 (Conv2D)             (None, 32, 32, 64)   1088        activation_113[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_37 (Add)                    (None, 32, 32, 64)   0           conv2d_121[0][0]                 \n",
            "                                                                 conv2d_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_114 (BatchN (None, 32, 32, 64)   256         add_37[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_114 (Dropout)           (None, 32, 32, 64)   0           batch_normalization_114[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_114 (Activation)     (None, 32, 32, 64)   0           dropout_114[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_122 (Conv2D)             (None, 32, 32, 16)   1040        activation_114[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_115 (BatchN (None, 32, 32, 16)   64          conv2d_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_115 (Dropout)           (None, 32, 32, 16)   0           batch_normalization_115[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_115 (Activation)     (None, 32, 32, 16)   0           dropout_115[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_123 (Conv2D)             (None, 32, 32, 16)   2320        activation_115[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_116 (BatchN (None, 32, 32, 16)   64          conv2d_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_116 (Dropout)           (None, 32, 32, 16)   0           batch_normalization_116[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_116 (Activation)     (None, 32, 32, 16)   0           dropout_116[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_124 (Conv2D)             (None, 32, 32, 64)   1088        activation_116[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_38 (Add)                    (None, 32, 32, 64)   0           add_37[0][0]                     \n",
            "                                                                 conv2d_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_117 (BatchN (None, 32, 32, 64)   256         add_38[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_117 (Dropout)           (None, 32, 32, 64)   0           batch_normalization_117[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_117 (Activation)     (None, 32, 32, 64)   0           dropout_117[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_125 (Conv2D)             (None, 32, 32, 16)   1040        activation_117[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_118 (BatchN (None, 32, 32, 16)   64          conv2d_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_118 (Dropout)           (None, 32, 32, 16)   0           batch_normalization_118[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_118 (Activation)     (None, 32, 32, 16)   0           dropout_118[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_126 (Conv2D)             (None, 32, 32, 16)   2320        activation_118[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_119 (BatchN (None, 32, 32, 16)   64          conv2d_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_119 (Dropout)           (None, 32, 32, 16)   0           batch_normalization_119[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_119 (Activation)     (None, 32, 32, 16)   0           dropout_119[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_127 (Conv2D)             (None, 32, 32, 64)   1088        activation_119[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_39 (Add)                    (None, 32, 32, 64)   0           add_38[0][0]                     \n",
            "                                                                 conv2d_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_120 (BatchN (None, 32, 32, 64)   256         add_39[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_120 (Dropout)           (None, 32, 32, 64)   0           batch_normalization_120[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_120 (Activation)     (None, 32, 32, 64)   0           dropout_120[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_128 (Conv2D)             (None, 32, 32, 16)   1040        activation_120[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_121 (BatchN (None, 32, 32, 16)   64          conv2d_128[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_121 (Dropout)           (None, 32, 32, 16)   0           batch_normalization_121[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_121 (Activation)     (None, 32, 32, 16)   0           dropout_121[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_129 (Conv2D)             (None, 32, 32, 16)   2320        activation_121[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_122 (BatchN (None, 32, 32, 16)   64          conv2d_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_122 (Dropout)           (None, 32, 32, 16)   0           batch_normalization_122[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_122 (Activation)     (None, 32, 32, 16)   0           dropout_122[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_130 (Conv2D)             (None, 32, 32, 64)   1088        activation_122[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_40 (Add)                    (None, 32, 32, 64)   0           add_39[0][0]                     \n",
            "                                                                 conv2d_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_123 (BatchN (None, 32, 32, 64)   256         add_40[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_123 (Dropout)           (None, 32, 32, 64)   0           batch_normalization_123[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_123 (Activation)     (None, 32, 32, 64)   0           dropout_123[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_131 (Conv2D)             (None, 32, 32, 16)   1040        activation_123[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_124 (BatchN (None, 32, 32, 16)   64          conv2d_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_124 (Dropout)           (None, 32, 32, 16)   0           batch_normalization_124[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_124 (Activation)     (None, 32, 32, 16)   0           dropout_124[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_132 (Conv2D)             (None, 32, 32, 16)   2320        activation_124[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_125 (BatchN (None, 32, 32, 16)   64          conv2d_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_125 (Dropout)           (None, 32, 32, 16)   0           batch_normalization_125[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_125 (Activation)     (None, 32, 32, 16)   0           dropout_125[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_133 (Conv2D)             (None, 32, 32, 64)   1088        activation_125[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_41 (Add)                    (None, 32, 32, 64)   0           add_40[0][0]                     \n",
            "                                                                 conv2d_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_126 (BatchN (None, 32, 32, 64)   256         add_41[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_126 (Dropout)           (None, 32, 32, 64)   0           batch_normalization_126[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_126 (Activation)     (None, 32, 32, 64)   0           dropout_126[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_134 (Conv2D)             (None, 32, 32, 16)   1040        activation_126[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_127 (BatchN (None, 32, 32, 16)   64          conv2d_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_127 (Dropout)           (None, 32, 32, 16)   0           batch_normalization_127[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_127 (Activation)     (None, 32, 32, 16)   0           dropout_127[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_135 (Conv2D)             (None, 32, 32, 16)   2320        activation_127[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_128 (BatchN (None, 32, 32, 16)   64          conv2d_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_128 (Dropout)           (None, 32, 32, 16)   0           batch_normalization_128[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_128 (Activation)     (None, 32, 32, 16)   0           dropout_128[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_136 (Conv2D)             (None, 32, 32, 64)   1088        activation_128[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_42 (Add)                    (None, 32, 32, 64)   0           add_41[0][0]                     \n",
            "                                                                 conv2d_136[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_129 (BatchN (None, 32, 32, 64)   256         add_42[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_129 (Dropout)           (None, 32, 32, 64)   0           batch_normalization_129[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_129 (Activation)     (None, 32, 32, 64)   0           dropout_129[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_137 (Conv2D)             (None, 16, 16, 64)   4160        activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_130 (BatchN (None, 16, 16, 64)   256         conv2d_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_130 (Dropout)           (None, 16, 16, 64)   0           batch_normalization_130[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_130 (Activation)     (None, 16, 16, 64)   0           dropout_130[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_138 (Conv2D)             (None, 16, 16, 64)   36928       activation_130[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_131 (BatchN (None, 16, 16, 64)   256         conv2d_138[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_131 (Dropout)           (None, 16, 16, 64)   0           batch_normalization_131[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_131 (Activation)     (None, 16, 16, 64)   0           dropout_131[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_140 (Conv2D)             (None, 16, 16, 128)  8320        add_42[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_139 (Conv2D)             (None, 16, 16, 128)  8320        activation_131[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_43 (Add)                    (None, 16, 16, 128)  0           conv2d_140[0][0]                 \n",
            "                                                                 conv2d_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_132 (BatchN (None, 16, 16, 128)  512         add_43[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_132 (Dropout)           (None, 16, 16, 128)  0           batch_normalization_132[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_132 (Activation)     (None, 16, 16, 128)  0           dropout_132[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_141 (Conv2D)             (None, 16, 16, 64)   8256        activation_132[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_133 (BatchN (None, 16, 16, 64)   256         conv2d_141[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_133 (Dropout)           (None, 16, 16, 64)   0           batch_normalization_133[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_133 (Activation)     (None, 16, 16, 64)   0           dropout_133[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_142 (Conv2D)             (None, 16, 16, 64)   36928       activation_133[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_134 (BatchN (None, 16, 16, 64)   256         conv2d_142[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_134 (Dropout)           (None, 16, 16, 64)   0           batch_normalization_134[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_134 (Activation)     (None, 16, 16, 64)   0           dropout_134[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_143 (Conv2D)             (None, 16, 16, 128)  8320        activation_134[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_44 (Add)                    (None, 16, 16, 128)  0           add_43[0][0]                     \n",
            "                                                                 conv2d_143[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_135 (BatchN (None, 16, 16, 128)  512         add_44[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_135 (Dropout)           (None, 16, 16, 128)  0           batch_normalization_135[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_135 (Activation)     (None, 16, 16, 128)  0           dropout_135[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_144 (Conv2D)             (None, 16, 16, 64)   8256        activation_135[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_136 (BatchN (None, 16, 16, 64)   256         conv2d_144[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_136 (Dropout)           (None, 16, 16, 64)   0           batch_normalization_136[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_136 (Activation)     (None, 16, 16, 64)   0           dropout_136[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_145 (Conv2D)             (None, 16, 16, 64)   36928       activation_136[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_137 (BatchN (None, 16, 16, 64)   256         conv2d_145[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_137 (Dropout)           (None, 16, 16, 64)   0           batch_normalization_137[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_137 (Activation)     (None, 16, 16, 64)   0           dropout_137[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_146 (Conv2D)             (None, 16, 16, 128)  8320        activation_137[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_45 (Add)                    (None, 16, 16, 128)  0           add_44[0][0]                     \n",
            "                                                                 conv2d_146[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_138 (BatchN (None, 16, 16, 128)  512         add_45[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_138 (Dropout)           (None, 16, 16, 128)  0           batch_normalization_138[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_138 (Activation)     (None, 16, 16, 128)  0           dropout_138[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_147 (Conv2D)             (None, 16, 16, 64)   8256        activation_138[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_139 (BatchN (None, 16, 16, 64)   256         conv2d_147[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_139 (Dropout)           (None, 16, 16, 64)   0           batch_normalization_139[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_139 (Activation)     (None, 16, 16, 64)   0           dropout_139[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_148 (Conv2D)             (None, 16, 16, 64)   36928       activation_139[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_140 (BatchN (None, 16, 16, 64)   256         conv2d_148[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_140 (Dropout)           (None, 16, 16, 64)   0           batch_normalization_140[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_140 (Activation)     (None, 16, 16, 64)   0           dropout_140[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_149 (Conv2D)             (None, 16, 16, 128)  8320        activation_140[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_46 (Add)                    (None, 16, 16, 128)  0           add_45[0][0]                     \n",
            "                                                                 conv2d_149[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_141 (BatchN (None, 16, 16, 128)  512         add_46[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_141 (Dropout)           (None, 16, 16, 128)  0           batch_normalization_141[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_141 (Activation)     (None, 16, 16, 128)  0           dropout_141[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_150 (Conv2D)             (None, 16, 16, 64)   8256        activation_141[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_142 (BatchN (None, 16, 16, 64)   256         conv2d_150[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_142 (Dropout)           (None, 16, 16, 64)   0           batch_normalization_142[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_142 (Activation)     (None, 16, 16, 64)   0           dropout_142[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_151 (Conv2D)             (None, 16, 16, 64)   36928       activation_142[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_143 (BatchN (None, 16, 16, 64)   256         conv2d_151[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_143 (Dropout)           (None, 16, 16, 64)   0           batch_normalization_143[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_143 (Activation)     (None, 16, 16, 64)   0           dropout_143[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_152 (Conv2D)             (None, 16, 16, 128)  8320        activation_143[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_47 (Add)                    (None, 16, 16, 128)  0           add_46[0][0]                     \n",
            "                                                                 conv2d_152[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_144 (BatchN (None, 16, 16, 128)  512         add_47[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_144 (Dropout)           (None, 16, 16, 128)  0           batch_normalization_144[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_144 (Activation)     (None, 16, 16, 128)  0           dropout_144[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_153 (Conv2D)             (None, 16, 16, 64)   8256        activation_144[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_145 (BatchN (None, 16, 16, 64)   256         conv2d_153[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_145 (Dropout)           (None, 16, 16, 64)   0           batch_normalization_145[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_145 (Activation)     (None, 16, 16, 64)   0           dropout_145[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_154 (Conv2D)             (None, 16, 16, 64)   36928       activation_145[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_146 (BatchN (None, 16, 16, 64)   256         conv2d_154[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_146 (Dropout)           (None, 16, 16, 64)   0           batch_normalization_146[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_146 (Activation)     (None, 16, 16, 64)   0           dropout_146[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_155 (Conv2D)             (None, 16, 16, 128)  8320        activation_146[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_48 (Add)                    (None, 16, 16, 128)  0           add_47[0][0]                     \n",
            "                                                                 conv2d_155[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_147 (BatchN (None, 16, 16, 128)  512         add_48[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_147 (Dropout)           (None, 16, 16, 128)  0           batch_normalization_147[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_147 (Activation)     (None, 16, 16, 128)  0           dropout_147[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_156 (Conv2D)             (None, 8, 8, 128)    16512       activation_147[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_148 (BatchN (None, 8, 8, 128)    512         conv2d_156[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_148 (Dropout)           (None, 8, 8, 128)    0           batch_normalization_148[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_148 (Activation)     (None, 8, 8, 128)    0           dropout_148[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_157 (Conv2D)             (None, 8, 8, 128)    147584      activation_148[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_149 (BatchN (None, 8, 8, 128)    512         conv2d_157[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_149 (Dropout)           (None, 8, 8, 128)    0           batch_normalization_149[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_149 (Activation)     (None, 8, 8, 128)    0           dropout_149[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_159 (Conv2D)             (None, 8, 8, 256)    33024       add_48[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_158 (Conv2D)             (None, 8, 8, 256)    33024       activation_149[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_49 (Add)                    (None, 8, 8, 256)    0           conv2d_159[0][0]                 \n",
            "                                                                 conv2d_158[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_150 (BatchN (None, 8, 8, 256)    1024        add_49[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_150 (Dropout)           (None, 8, 8, 256)    0           batch_normalization_150[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_150 (Activation)     (None, 8, 8, 256)    0           dropout_150[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_160 (Conv2D)             (None, 8, 8, 128)    32896       activation_150[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_151 (BatchN (None, 8, 8, 128)    512         conv2d_160[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_151 (Dropout)           (None, 8, 8, 128)    0           batch_normalization_151[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_151 (Activation)     (None, 8, 8, 128)    0           dropout_151[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_161 (Conv2D)             (None, 8, 8, 128)    147584      activation_151[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_152 (BatchN (None, 8, 8, 128)    512         conv2d_161[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_152 (Dropout)           (None, 8, 8, 128)    0           batch_normalization_152[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_152 (Activation)     (None, 8, 8, 128)    0           dropout_152[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_162 (Conv2D)             (None, 8, 8, 256)    33024       activation_152[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_50 (Add)                    (None, 8, 8, 256)    0           add_49[0][0]                     \n",
            "                                                                 conv2d_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_153 (BatchN (None, 8, 8, 256)    1024        add_50[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_153 (Dropout)           (None, 8, 8, 256)    0           batch_normalization_153[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_153 (Activation)     (None, 8, 8, 256)    0           dropout_153[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_163 (Conv2D)             (None, 8, 8, 128)    32896       activation_153[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_154 (BatchN (None, 8, 8, 128)    512         conv2d_163[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_154 (Dropout)           (None, 8, 8, 128)    0           batch_normalization_154[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_154 (Activation)     (None, 8, 8, 128)    0           dropout_154[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_164 (Conv2D)             (None, 8, 8, 128)    147584      activation_154[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_155 (BatchN (None, 8, 8, 128)    512         conv2d_164[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_155 (Dropout)           (None, 8, 8, 128)    0           batch_normalization_155[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_155 (Activation)     (None, 8, 8, 128)    0           dropout_155[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_165 (Conv2D)             (None, 8, 8, 256)    33024       activation_155[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_51 (Add)                    (None, 8, 8, 256)    0           add_50[0][0]                     \n",
            "                                                                 conv2d_165[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_156 (BatchN (None, 8, 8, 256)    1024        add_51[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_156 (Dropout)           (None, 8, 8, 256)    0           batch_normalization_156[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_156 (Activation)     (None, 8, 8, 256)    0           dropout_156[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_166 (Conv2D)             (None, 8, 8, 128)    32896       activation_156[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_157 (BatchN (None, 8, 8, 128)    512         conv2d_166[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_157 (Dropout)           (None, 8, 8, 128)    0           batch_normalization_157[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_157 (Activation)     (None, 8, 8, 128)    0           dropout_157[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_167 (Conv2D)             (None, 8, 8, 128)    147584      activation_157[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_158 (BatchN (None, 8, 8, 128)    512         conv2d_167[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_158 (Dropout)           (None, 8, 8, 128)    0           batch_normalization_158[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_158 (Activation)     (None, 8, 8, 128)    0           dropout_158[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_168 (Conv2D)             (None, 8, 8, 256)    33024       activation_158[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_52 (Add)                    (None, 8, 8, 256)    0           add_51[0][0]                     \n",
            "                                                                 conv2d_168[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_159 (BatchN (None, 8, 8, 256)    1024        add_52[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_159 (Dropout)           (None, 8, 8, 256)    0           batch_normalization_159[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_159 (Activation)     (None, 8, 8, 256)    0           dropout_159[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_169 (Conv2D)             (None, 8, 8, 128)    32896       activation_159[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_160 (BatchN (None, 8, 8, 128)    512         conv2d_169[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_160 (Dropout)           (None, 8, 8, 128)    0           batch_normalization_160[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_160 (Activation)     (None, 8, 8, 128)    0           dropout_160[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_170 (Conv2D)             (None, 8, 8, 128)    147584      activation_160[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_161 (BatchN (None, 8, 8, 128)    512         conv2d_170[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_161 (Dropout)           (None, 8, 8, 128)    0           batch_normalization_161[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_161 (Activation)     (None, 8, 8, 128)    0           dropout_161[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_171 (Conv2D)             (None, 8, 8, 256)    33024       activation_161[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_53 (Add)                    (None, 8, 8, 256)    0           add_52[0][0]                     \n",
            "                                                                 conv2d_171[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_162 (BatchN (None, 8, 8, 256)    1024        add_53[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_162 (Dropout)           (None, 8, 8, 256)    0           batch_normalization_162[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_162 (Activation)     (None, 8, 8, 256)    0           dropout_162[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_172 (Conv2D)             (None, 8, 8, 128)    32896       activation_162[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_163 (BatchN (None, 8, 8, 128)    512         conv2d_172[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_163 (Dropout)           (None, 8, 8, 128)    0           batch_normalization_163[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_163 (Activation)     (None, 8, 8, 128)    0           dropout_163[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_173 (Conv2D)             (None, 8, 8, 128)    147584      activation_163[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_164 (BatchN (None, 8, 8, 128)    512         conv2d_173[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_164 (Dropout)           (None, 8, 8, 128)    0           batch_normalization_164[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_164 (Activation)     (None, 8, 8, 128)    0           dropout_164[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_174 (Conv2D)             (None, 8, 8, 256)    33024       activation_164[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_54 (Add)                    (None, 8, 8, 256)    0           add_53[0][0]                     \n",
            "                                                                 conv2d_174[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_165 (BatchN (None, 8, 8, 256)    1024        add_54[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_165 (Activation)     (None, 8, 8, 256)    0           batch_normalization_165[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 1, 1, 256)    0           activation_165[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_165 (Dropout)           (None, 1, 1, 256)    0           average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 256)          0           dropout_165[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 10)           2570        flatten_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,673,738\n",
            "Trainable params: 1,663,338\n",
            "Non-trainable params: 10,400\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6SbOo8hOTzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit the model\n",
        "modelo = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=75, batch_size=32)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        " \n",
        "print(\"Error: %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz1upCKlvX28",
        "colab_type": "code",
        "outputId": "6346672b-77e7-4348-dc3c-2883bb19d932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=15,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # set range for random shear\n",
        "        shear_range=0.,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "# Compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "modelo = model.fit_generator(datagen.flow(X_train, y_train, batch_size=64),\n",
        "                        validation_data=(X_test, y_test),\n",
        "                        epochs=100, verbose=1)\n",
        "    \n",
        "\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "\n",
        "plt.plot(modelo.history['loss'])\n",
        "plt.plot(modelo.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()\n",
        "\n",
        "print(\"Error: %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "782/782 [==============================] - 258s 330ms/step - loss: 2.3077 - accuracy: 0.4427 - val_loss: 2.0860 - val_accuracy: 0.4987\n",
            "Epoch 2/100\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 1.6765 - accuracy: 0.5666 - val_loss: 1.6006 - val_accuracy: 0.6030\n",
            "Epoch 3/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 1.4534 - accuracy: 0.6124 - val_loss: 1.5157 - val_accuracy: 0.5850\n",
            "Epoch 4/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 1.3152 - accuracy: 0.6442 - val_loss: 1.3462 - val_accuracy: 0.6463\n",
            "Epoch 5/100\n",
            "782/782 [==============================] - 236s 302ms/step - loss: 1.2223 - accuracy: 0.6705 - val_loss: 1.4322 - val_accuracy: 0.6174\n",
            "Epoch 6/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 1.1439 - accuracy: 0.6940 - val_loss: 1.1589 - val_accuracy: 0.7083\n",
            "Epoch 7/100\n",
            "782/782 [==============================] - 238s 305ms/step - loss: 1.0832 - accuracy: 0.7113 - val_loss: 1.0427 - val_accuracy: 0.7282\n",
            "Epoch 8/100\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 1.0304 - accuracy: 0.7262 - val_loss: 1.0437 - val_accuracy: 0.7399\n",
            "Epoch 9/100\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.9941 - accuracy: 0.7376 - val_loss: 1.0044 - val_accuracy: 0.7468\n",
            "Epoch 10/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.9620 - accuracy: 0.7487 - val_loss: 1.0003 - val_accuracy: 0.7415\n",
            "Epoch 11/100\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.9296 - accuracy: 0.7585 - val_loss: 1.0109 - val_accuracy: 0.7485\n",
            "Epoch 12/100\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.9075 - accuracy: 0.7653 - val_loss: 1.0018 - val_accuracy: 0.7488\n",
            "Epoch 13/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.8792 - accuracy: 0.7744 - val_loss: 0.9142 - val_accuracy: 0.7799\n",
            "Epoch 14/100\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.8573 - accuracy: 0.7794 - val_loss: 0.9676 - val_accuracy: 0.7564\n",
            "Epoch 15/100\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.8388 - accuracy: 0.7863 - val_loss: 0.9464 - val_accuracy: 0.7699\n",
            "Epoch 16/100\n",
            "782/782 [==============================] - 238s 305ms/step - loss: 0.8234 - accuracy: 0.7910 - val_loss: 0.8511 - val_accuracy: 0.7946\n",
            "Epoch 17/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.8054 - accuracy: 0.7962 - val_loss: 0.8487 - val_accuracy: 0.7924\n",
            "Epoch 18/100\n",
            "782/782 [==============================] - 240s 307ms/step - loss: 0.7908 - accuracy: 0.7997 - val_loss: 0.8003 - val_accuracy: 0.8045\n",
            "Epoch 19/100\n",
            "782/782 [==============================] - 240s 307ms/step - loss: 0.7826 - accuracy: 0.8041 - val_loss: 0.8454 - val_accuracy: 0.8014\n",
            "Epoch 20/100\n",
            "782/782 [==============================] - 240s 307ms/step - loss: 0.7675 - accuracy: 0.8068 - val_loss: 0.8939 - val_accuracy: 0.7847\n",
            "Epoch 21/100\n",
            "782/782 [==============================] - 240s 307ms/step - loss: 0.7519 - accuracy: 0.8124 - val_loss: 1.0997 - val_accuracy: 0.7481\n",
            "Epoch 22/100\n",
            "782/782 [==============================] - 239s 306ms/step - loss: 0.7426 - accuracy: 0.8160 - val_loss: 0.9212 - val_accuracy: 0.7837\n",
            "Epoch 23/100\n",
            "782/782 [==============================] - 240s 306ms/step - loss: 0.7334 - accuracy: 0.8167 - val_loss: 0.7689 - val_accuracy: 0.8167\n",
            "Epoch 24/100\n",
            "782/782 [==============================] - 239s 306ms/step - loss: 0.7208 - accuracy: 0.8236 - val_loss: 1.1969 - val_accuracy: 0.7411\n",
            "Epoch 25/100\n",
            "782/782 [==============================] - 238s 305ms/step - loss: 0.7136 - accuracy: 0.8245 - val_loss: 0.9208 - val_accuracy: 0.7955\n",
            "Epoch 26/100\n",
            "782/782 [==============================] - 238s 305ms/step - loss: 0.6994 - accuracy: 0.8292 - val_loss: 0.7551 - val_accuracy: 0.8191\n",
            "Epoch 27/100\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.6967 - accuracy: 0.8292 - val_loss: 1.3185 - val_accuracy: 0.7065\n",
            "Epoch 28/100\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.6908 - accuracy: 0.8301 - val_loss: 0.8554 - val_accuracy: 0.7924\n",
            "Epoch 29/100\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.6773 - accuracy: 0.8369 - val_loss: 0.9154 - val_accuracy: 0.7703\n",
            "Epoch 30/100\n",
            "782/782 [==============================] - 239s 305ms/step - loss: 0.6760 - accuracy: 0.8354 - val_loss: 0.7811 - val_accuracy: 0.8165\n",
            "Epoch 31/100\n",
            "782/782 [==============================] - 240s 307ms/step - loss: 0.6688 - accuracy: 0.8395 - val_loss: 1.3115 - val_accuracy: 0.7510\n",
            "Epoch 32/100\n",
            "782/782 [==============================] - 240s 307ms/step - loss: 0.6604 - accuracy: 0.8415 - val_loss: 0.8889 - val_accuracy: 0.7852\n",
            "Epoch 33/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.6546 - accuracy: 0.8423 - val_loss: 0.7516 - val_accuracy: 0.8271\n",
            "Epoch 34/100\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.6496 - accuracy: 0.8449 - val_loss: 0.8174 - val_accuracy: 0.8128\n",
            "Epoch 35/100\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.6471 - accuracy: 0.8446 - val_loss: 0.7481 - val_accuracy: 0.8341\n",
            "Epoch 36/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.6391 - accuracy: 0.8477 - val_loss: 1.1478 - val_accuracy: 0.7566\n",
            "Epoch 37/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.6340 - accuracy: 0.8493 - val_loss: 0.7588 - val_accuracy: 0.8207\n",
            "Epoch 38/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.6302 - accuracy: 0.8508 - val_loss: 0.7600 - val_accuracy: 0.8321\n",
            "Epoch 39/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.6269 - accuracy: 0.8531 - val_loss: 0.9276 - val_accuracy: 0.7935\n",
            "Epoch 40/100\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.6219 - accuracy: 0.8519 - val_loss: 0.7824 - val_accuracy: 0.8225\n",
            "Epoch 41/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.6203 - accuracy: 0.8527 - val_loss: 0.8289 - val_accuracy: 0.8050\n",
            "Epoch 42/100\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.6111 - accuracy: 0.8572 - val_loss: 0.7704 - val_accuracy: 0.8264\n",
            "Epoch 43/100\n",
            "782/782 [==============================] - 236s 302ms/step - loss: 0.6093 - accuracy: 0.8574 - val_loss: 0.7529 - val_accuracy: 0.8249\n",
            "Epoch 44/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.6056 - accuracy: 0.8592 - val_loss: 0.7920 - val_accuracy: 0.8098\n",
            "Epoch 45/100\n",
            "782/782 [==============================] - 236s 302ms/step - loss: 0.6030 - accuracy: 0.8584 - val_loss: 0.7289 - val_accuracy: 0.8361\n",
            "Epoch 46/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.5998 - accuracy: 0.8596 - val_loss: 0.9553 - val_accuracy: 0.7793\n",
            "Epoch 47/100\n",
            "782/782 [==============================] - 236s 302ms/step - loss: 0.5959 - accuracy: 0.8609 - val_loss: 0.9830 - val_accuracy: 0.7852\n",
            "Epoch 48/100\n",
            "782/782 [==============================] - 236s 302ms/step - loss: 0.5903 - accuracy: 0.8627 - val_loss: 0.6810 - val_accuracy: 0.8537\n",
            "Epoch 49/100\n",
            "782/782 [==============================] - 236s 302ms/step - loss: 0.5858 - accuracy: 0.8648 - val_loss: 0.7050 - val_accuracy: 0.8348\n",
            "Epoch 50/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.5895 - accuracy: 0.8640 - val_loss: 0.8563 - val_accuracy: 0.8219\n",
            "Epoch 51/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.5874 - accuracy: 0.8641 - val_loss: 0.6595 - val_accuracy: 0.8497\n",
            "Epoch 52/100\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.5804 - accuracy: 0.8668 - val_loss: 1.1767 - val_accuracy: 0.7530\n",
            "Epoch 53/100\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.5813 - accuracy: 0.8679 - val_loss: 0.7218 - val_accuracy: 0.8375\n",
            "Epoch 54/100\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.5753 - accuracy: 0.8683 - val_loss: 0.7738 - val_accuracy: 0.8214\n",
            "Epoch 55/100\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.5781 - accuracy: 0.8675 - val_loss: 0.7802 - val_accuracy: 0.8251\n",
            "Epoch 56/100\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.5725 - accuracy: 0.8705 - val_loss: 0.7585 - val_accuracy: 0.8327\n",
            "Epoch 57/100\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.5635 - accuracy: 0.8724 - val_loss: 0.7613 - val_accuracy: 0.8390\n",
            "Epoch 58/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.5732 - accuracy: 0.8697 - val_loss: 0.6948 - val_accuracy: 0.8485\n",
            "Epoch 59/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.5675 - accuracy: 0.8696 - val_loss: 0.7301 - val_accuracy: 0.8324\n",
            "Epoch 60/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.5641 - accuracy: 0.8713 - val_loss: 0.8243 - val_accuracy: 0.8141\n",
            "Epoch 61/100\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.5590 - accuracy: 0.8728 - val_loss: 0.6873 - val_accuracy: 0.8502\n",
            "Epoch 62/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.5609 - accuracy: 0.8732 - val_loss: 0.6383 - val_accuracy: 0.8601\n",
            "Epoch 63/100\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.5586 - accuracy: 0.8741 - val_loss: 0.7652 - val_accuracy: 0.8304\n",
            "Epoch 64/100\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.5595 - accuracy: 0.8733 - val_loss: 0.8550 - val_accuracy: 0.8172\n",
            "Epoch 65/100\n",
            "782/782 [==============================] - 237s 302ms/step - loss: 0.5540 - accuracy: 0.8748 - val_loss: 0.6129 - val_accuracy: 0.8683\n",
            "Epoch 66/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.5501 - accuracy: 0.8760 - val_loss: 0.7688 - val_accuracy: 0.8303\n",
            "Epoch 67/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.5474 - accuracy: 0.8782 - val_loss: 0.7630 - val_accuracy: 0.8326\n",
            "Epoch 68/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.5447 - accuracy: 0.8790 - val_loss: 0.7768 - val_accuracy: 0.8261\n",
            "Epoch 69/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.5432 - accuracy: 0.8791 - val_loss: 0.7492 - val_accuracy: 0.8458\n",
            "Epoch 70/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.5469 - accuracy: 0.8770 - val_loss: 0.7298 - val_accuracy: 0.8498\n",
            "Epoch 71/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.5456 - accuracy: 0.8781 - val_loss: 0.7887 - val_accuracy: 0.8190\n",
            "Epoch 72/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.5393 - accuracy: 0.8785 - val_loss: 0.9407 - val_accuracy: 0.7992\n",
            "Epoch 73/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.5410 - accuracy: 0.8795 - val_loss: 0.7443 - val_accuracy: 0.8342\n",
            "Epoch 74/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.5372 - accuracy: 0.8804 - val_loss: 0.7460 - val_accuracy: 0.8344\n",
            "Epoch 75/100\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.5332 - accuracy: 0.8814 - val_loss: 0.8600 - val_accuracy: 0.8191\n",
            "Epoch 76/100\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.5310 - accuracy: 0.8831 - val_loss: 0.6441 - val_accuracy: 0.8697\n",
            "Epoch 77/100\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.5326 - accuracy: 0.8830 - val_loss: 0.7964 - val_accuracy: 0.8310\n",
            "Epoch 78/100\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.5314 - accuracy: 0.8815 - val_loss: 0.7314 - val_accuracy: 0.8373\n",
            "Epoch 79/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.5268 - accuracy: 0.8838 - val_loss: 0.8092 - val_accuracy: 0.8333\n",
            "Epoch 80/100\n",
            "782/782 [==============================] - 238s 305ms/step - loss: 0.5355 - accuracy: 0.8816 - val_loss: 0.6379 - val_accuracy: 0.8692\n",
            "Epoch 81/100\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.5317 - accuracy: 0.8816 - val_loss: 0.9131 - val_accuracy: 0.8048\n",
            "Epoch 82/100\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.5281 - accuracy: 0.8832 - val_loss: 0.5866 - val_accuracy: 0.8763\n",
            "Epoch 83/100\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.5289 - accuracy: 0.8829 - val_loss: 0.9581 - val_accuracy: 0.8122\n",
            "Epoch 84/100\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.5239 - accuracy: 0.8856 - val_loss: 0.6853 - val_accuracy: 0.8534\n",
            "Epoch 85/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.5242 - accuracy: 0.8849 - val_loss: 0.6574 - val_accuracy: 0.8608\n",
            "Epoch 86/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.5208 - accuracy: 0.8851 - val_loss: 0.6946 - val_accuracy: 0.8529\n",
            "Epoch 87/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.5239 - accuracy: 0.8852 - val_loss: 0.6790 - val_accuracy: 0.8549\n",
            "Epoch 88/100\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.5176 - accuracy: 0.8865 - val_loss: 0.6164 - val_accuracy: 0.8710\n",
            "Epoch 89/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.5163 - accuracy: 0.8882 - val_loss: 0.8589 - val_accuracy: 0.8093\n",
            "Epoch 90/100\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.5176 - accuracy: 0.8869 - val_loss: 0.7237 - val_accuracy: 0.8482\n",
            "Epoch 91/100\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.5195 - accuracy: 0.8863 - val_loss: 0.6606 - val_accuracy: 0.8644\n",
            "Epoch 92/100\n",
            "782/782 [==============================] - 237s 303ms/step - loss: 0.5186 - accuracy: 0.8882 - val_loss: 0.7763 - val_accuracy: 0.8249\n",
            "Epoch 93/100\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.5135 - accuracy: 0.8906 - val_loss: 0.6893 - val_accuracy: 0.8627\n",
            "Epoch 94/100\n",
            "782/782 [==============================] - 237s 304ms/step - loss: 0.5145 - accuracy: 0.8886 - val_loss: 0.6722 - val_accuracy: 0.8593\n",
            "Epoch 95/100\n",
            "782/782 [==============================] - 238s 305ms/step - loss: 0.5103 - accuracy: 0.8891 - val_loss: 0.7019 - val_accuracy: 0.8494\n",
            "Epoch 96/100\n",
            "782/782 [==============================] - 240s 306ms/step - loss: 0.5152 - accuracy: 0.8895 - val_loss: 0.7068 - val_accuracy: 0.8529\n",
            "Epoch 97/100\n",
            "782/782 [==============================] - 238s 304ms/step - loss: 0.5156 - accuracy: 0.8889 - val_loss: 0.7686 - val_accuracy: 0.8450\n",
            "Epoch 98/100\n",
            "782/782 [==============================] - 238s 305ms/step - loss: 0.5052 - accuracy: 0.8908 - val_loss: 1.0385 - val_accuracy: 0.7930\n",
            "Epoch 99/100\n",
            "782/782 [==============================] - 238s 305ms/step - loss: 0.5077 - accuracy: 0.8907 - val_loss: 0.7040 - val_accuracy: 0.8553\n",
            "Epoch 100/100\n",
            "782/782 [==============================] - 238s 305ms/step - loss: 0.5097 - accuracy: 0.8895 - val_loss: 1.0706 - val_accuracy: 0.7516\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nICIiolEKRERENEqBiIiIRv8B8nZuGOhi+OAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Error: 24.84%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}